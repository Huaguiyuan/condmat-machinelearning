{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum wavefunctions as restricted Boltzmann machines (RBM)\n",
    "(This neural network ansatz was introduced in paper [Solving the quantum many-body problem with artificial neural networks](http://doi.org/10.1126/science.aag2302).)\n",
    "\n",
    "The generic $N$-spin wavefunction in the Ising basis is \n",
    "$$\n",
    "|\\psi\\rangle = \\sum_{\\{\\sigma\\}}\\Psi(\\sigma_1, \\dots, \\sigma_N) |\\sigma_1, \\dots, \\sigma_N \\rangle\n",
    "$$\n",
    "The RBM ansatz defines a set of $M$ hidden binary variable $h_j$ and the wavefunction elements are given based on a set of complex-valued weights $w_{ij}$, between the hidden and visible spins, and corresponding biases (local fields) $a_i$ and $b_j$ for the visible and hidden spins respecitively.\n",
    "$$\n",
    "    \\Psi(\\{\\sigma\\}, \\{h\\}) = \\frac{1}{Z} \\exp{\\left(\\sum_i a_i \\sigma_i + \\sum_{ij} w_{ij}\\sigma_ih_j + \\sum_j b_j h_j\\right)}\n",
    "$$\n",
    "This means we are charactrising a linear map from a set of $2^N$ spin configurations to a complex number by $N\\times M+N+M$ variables.\n",
    "\n",
    "For a given set of weights $w'$ and biases $b'$ integrating the the hidden layer variables we get\n",
    "$$\n",
    "    \\Psi(\\{\\sigma\\}) = \\frac{1}{Z} \\exp{\\left(\\sum_i a_i \\sigma_i\\right)} \\times  \\prod_j 2\\cosh(\\theta_j) \\quad \\text{where} \\quad\n",
    "    \\theta_j = \\sum_{i} w'_{ij}\\sigma_i + b'_j\n",
    "$$\n",
    "\n",
    "Note that the variable $Z$ is just introduced for normalization (we will ignore it throughtout because we explicitely normalized the wavefunctions and expectation values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psi = wavefunction(s, a, θs) = 21.38825288305486 + 555.3118727599374im\n",
      "s = [-1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "### Parameters\n",
    "# `N` is the number of spins, `M` the number of hidden spins and `α=M/N` the ratio of hidden layer to visible layer\n",
    "N = 16\n",
    "α = 2\n",
    "M = floor(α * N)\n",
    "n_pars = N*M + N + M\n",
    "\n",
    "# The weights of connections and biases for visible and hidden layer\n",
    "scale = 0.1\n",
    "w = scale * randn(ComplexF64, N, M)\n",
    "#w = zeros(ComplexF64, N, M)\n",
    "a = randn(ComplexF64, N)\n",
    "#a = fill(1.0im, N)\n",
    "b = randn(ComplexF64, M)\n",
    "#b = zeros(ComplexF64, M)\n",
    "\n",
    "# pick a random configuration\n",
    "s = rand([1, -1], N)\n",
    "θs = transpose(w)*s + b\n",
    "\n",
    "# definition of the wavefunction\n",
    "wavefunction(s, a, θs) = exp(dot(s, a)) * prod(cosh.(θs))\n",
    "\n",
    "@show psi = wavefunction(s, a, θs)\n",
    "@show s;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The optimization algorithm\n",
    "### Gradient descent\n",
    "we would like to represent the ground state of some Hamiltonian $H$ (for example the quantum Ising in transverse field) so we choose to minimize the variational energy with respect to $\\psi$ that is a function of the current weights $w$ and biases $a, b$.\n",
    "$$\n",
    "    E = \\frac{\\langle \\psi | H | \\psi \\rangle}{\\langle \\psi | \\psi \\rangle}\n",
    "$$\n",
    "The generalized force (the gradient of the variational energy) is a real-valued function of the complex weights and biases, therefore\n",
    "$$\n",
    "    F_k(w) = \\frac{\\partial}{\\partial w_k} E(w) = 2 \\frac{\\partial}{\\partial w^*_k}E(w, w^*) =\n",
    "    \\sum_{\\boldsymbol{\\sigma}} \n",
    "    \\frac{\\left[\\frac{\\partial}{\\partial w^*_k} \\psi^*(\\boldsymbol{\\sigma} )\\right]\\langle \\boldsymbol{\\sigma}  | H | \\psi \\rangle}{\\langle \\psi | \\psi \\rangle} - \n",
    "    \\sum_\\boldsymbol{\\sigma}\n",
    "    \\frac{\\psi^*(\\boldsymbol{\\sigma} )\\langle \\boldsymbol{\\sigma}  | H | \\psi \\rangle}{\\langle \\psi | \\psi \\rangle} \n",
    "    \\sum_{\\boldsymbol{\\sigma} '} \n",
    "    \\frac{\\left[\\frac{\\partial}{\\partial w^*_k} \\psi^*(\\boldsymbol{\\sigma} ')\\right]\\psi(\\boldsymbol{\\sigma} ')}{\\langle \\psi | \\psi \\rangle}\n",
    "$$\n",
    "As usual Interpreting $p(\\boldsymbol{\\sigma} ) = \\frac{|\\psi(\\boldsymbol{\\sigma} )|^2}{\\langle \\psi | \\psi \\rangle}$ as probability of configuration $\\boldsymbol{\\sigma} $ and defining the average (with some abuse of notation)\n",
    "$\\langle A\\rangle = \\sum_{\\boldsymbol{\\sigma}} p(\\boldsymbol{\\sigma}) A(\\boldsymbol{\\sigma})$ we have \n",
    "$$\n",
    "F_k(w) =  \\langle O^*_k \\mathcal{E}\\rangle - \\langle O^*_k\\rangle\\langle \\mathcal{E}\\rangle\n",
    "$$\n",
    "where the operators $O_k(\\boldsymbol{\\sigma})$ and $\\mathcal{E}(\\boldsymbol{\\sigma})$ are defined as \n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\mathcal{E}(\\boldsymbol{\\sigma} ) & = \\frac{\\langle \\boldsymbol{\\sigma}  | H | \\psi \\rangle}{\\psi(\\boldsymbol{\\sigma})}  \\\\\n",
    "        O_k(\\boldsymbol{\\sigma}) & = \\frac{\\partial_k \\psi(\\boldsymbol{\\sigma})}{\\psi(\\boldsymbol{\\sigma})} \n",
    "    \\end{aligned}\n",
    "$$\n",
    "The usual gradient descent algorithm can be implemented to update the weights and biases with learning rate $\\lambda$ according to \n",
    "$$\n",
    "    w_k^{i+1} = w_k^i - \\lambda F_k(w_k^i)\n",
    "$$\n",
    "\n",
    "Based on the definition of $\\psi(\\boldsymbol{\\sigma})$, the derivatives are given by\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\partial_{a_i} \\ln \\psi &= \\sigma_i \\\\\n",
    "        \\partial_{b_j} \\ln \\psi &= \\tanh \\theta_j \\\\\n",
    "        \\partial_{w_{ij}} \\ln \\psi &= \\sigma_i \\tanh \\theta_j\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Monte Carlo evaluation\n",
    "Since the configuration space is $2^N$ large, it is generally impossible to evaluate the above averages for problems of interest. Therefore, we should pick a sampling method to approximately evaluate the averages. At each step of the optimization, the Monte-Carlo sampling method picks a series of random configurations $\\boldsymbol{\\sigma}_1, \\dots, \\boldsymbol{\\sigma}_\\ell$ and evaluates the operators. New configurations are chosen by flipping a random spin and according to the acceptance probability \n",
    "$$\n",
    "    P_{\\text{accept}}(\\boldsymbol{\\sigma}_{\\ell+1}) = \\min \\left(1, \\left|\\frac{\\psi(\\boldsymbol{\\sigma}_{\\ell+1})}{\\psi(\\boldsymbol{\\sigma}_\\ell)}\\right|^2\\right)\n",
    "$$\n",
    "To avoid recalculations, the set of $\\theta_j$ is saved and updated only for the connection to the flipped spin.\n",
    "\n",
    "Note that this process amount to a version of *stochastic* gradient descent as the total gradient is not computed at each step. The choice of configurations, however, is chosen by a Metropolis walk rather than uniform random to respect the probability distribution induced by the wavefunction over the configurations.\n",
    "\n",
    "### Stochastic reconfiguration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "λ = 0.05\n",
    "# Gradient descent RBM learning for ground state of quantum Ising transverse field\n",
    "for learning_step=1:100\n",
    "    g = 1.00\n",
    "    n_mcsweeps = 10\n",
    "    l = 0\n",
    "    O_av    = zeros(n_pars)\n",
    "    E_av    = 0.0\n",
    "    szsz_av = zeros(N)\n",
    "    sx_av   = zeros(N)\n",
    "    EO_av   = zeros(n_pars)\n",
    "    n_samples = n_mcsweeps*N\n",
    "    counter = 0\n",
    "    while l < n_samples\n",
    "        #print(\"x\")\n",
    "        spin = rand(1:N)\n",
    "        counter += 1\n",
    "        #spin = mod1(counter, N)\n",
    "        #println(spin)\n",
    "        s_new = copy(s)\n",
    "        s_new[spin] *= -1\n",
    "        θs_new = θs - 2 * s[spin] .* w[spin, :]\n",
    "        psi_new = wavefunction(s_new, a, θs_new)\n",
    "        #println(abs(psi_new/psi)^2)\n",
    "        if abs(psi_new/psi)^2 > rand()\n",
    "            psi = psi_new\n",
    "            s  = s_new\n",
    "            #println()\n",
    "            #println(s)\n",
    "            θs = θs_new\n",
    "            l += 1\n",
    "            O = conj([s...;tanh.(θs)...; [s[i] * tanh.(θs)[j] for j=1:M for i=1:N]...])\n",
    "            szsz = [s[i]*s[mod1(i+1, N)] for i in 1:N]\n",
    "            sx = []\n",
    "            for i=1:N\n",
    "                s_measure = copy(s)\n",
    "                s_measure[i] *= -1\n",
    "                #println(s-s_measure)\n",
    "                push!(sx, wavefunction(s_measure, a, θs - 2 * s[i] .* w[i, :])/psi)\n",
    "                #push!(sx, wavefunction(s_measure, a, θs)/psi)\n",
    "            end\n",
    "            #display(sx)\n",
    "            #sleep(10)\n",
    "            E = -sum(szsz) - g*sum(sx)\n",
    "            #println(E, sum(szsz))\n",
    "            O_av += O\n",
    "            #E_av += sum(szsz)\n",
    "            E_av += E\n",
    "            EO_av += E .* O\n",
    "        end\n",
    "    end\n",
    "\n",
    "    EO_av /= n_samples\n",
    "    O_av /= n_samples\n",
    "    E_av /= n_samples\n",
    "    F = EO_av - E_av .* O_av\n",
    "\n",
    "    println(E_av)\n",
    "    # update weights and bias\n",
    "    a -= λ *F[1:N]\n",
    "    b -= λ *F[N+1:N+M]\n",
    "    w -= λ *reshape(F[N+M+1:n_pars], N, M)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
