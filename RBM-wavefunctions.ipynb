{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum wavefunctions as restricted Boltzmann machines (RBM)\n",
    "(This neural network ansatz was introduced in paper [Solving the quantum many-body problem with artificial neural networks](http://doi.org/10.1126/science.aag2302).)\n",
    "\n",
    "The generic $N$-spin wavefunction in the Ising basis is \n",
    "$$\n",
    "\\lvert\\psi\\rangle = \\sum_{\\{\\sigma\\}}\\Psi(\\sigma_1, \\dots, \\sigma_N) \\lvert\\sigma_1, \\dots, \\sigma_N \\rangle\n",
    "$$\n",
    "The RBM ansatz defines a set of $M$ hidden binary variable $h_j$ and the wavefunction elements are given based on a set of complex-valued weights $w_{ij}$, between the hidden and visible spins, and corresponding biases (local fields) $a_i$ and $b_j$ for the visible and hidden spins respecitively.\n",
    "$$\n",
    "    \\Psi(\\{\\sigma\\}, \\{h\\}) = \\frac{1}{Z} \\exp{\\left(\\sum_i a_i \\sigma_i + \\sum_{ij} w_{ij}\\sigma_ih_j + \\sum_j b_j h_j\\right)}\n",
    "$$\n",
    "This means we are charactrising a linear map from a set of $2^N$ spin configurations to a complex number by $N\\times M+N+M$ variables.\n",
    "\n",
    "For a given set of weights $w'$ and biases $b'$ integrating the the hidden layer variables we get\n",
    "$$\n",
    "    \\Psi(\\{\\sigma\\}) = \\frac{1}{Z} \\exp{\\left(\\sum_i a_i \\sigma_i\\right)} \\times  \\prod_j 2\\cosh(\\theta_j) \\quad \\text{where} \\quad\n",
    "    \\theta_j = \\sum_{i} w'_{ij}\\sigma_i + b'_j\n",
    "$$\n",
    "\n",
    "Note that the variable $Z$ is just introduced for normalization (we will ignore it throughtout because we explicitely normalized the wavefunctions and expectation values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using QuadGK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Restricted Boltzmann machine quantum state Ansatz\"\n",
    "mutable struct QRBMState\n",
    "    N  :: Int\n",
    "    M  :: Int\n",
    "    a  :: Vector{ComplexF64}\n",
    "    b  :: Vector{ComplexF64}\n",
    "    w  :: Matrix{ComplexF64}\n",
    "    σs :: Vector{Int}\n",
    "    θs :: Vector{ComplexF64}\n",
    "    amp :: ComplexF64\n",
    "end\n",
    "\n",
    "function QRBMState(N::Int, M::Int)\n",
    "    scale = 0.1\n",
    "    w   = scale * randn(ComplexF64, N, M)\n",
    "    a   = randn(ComplexF64, N)\n",
    "    b   = randn(ComplexF64, M)\n",
    "    σs  = rand([1, -1], N)\n",
    "    θs  = transpose(w)*σs + b\n",
    "    amp = amplitude_rbmwf(σs, a, θs)\n",
    "    QRBMState(N, M, a, b, w, σs, θs, amp)\n",
    "end\n",
    "\n",
    "function recalc_θs!(state::QRBMState)\n",
    "    state.θs = transpose(state.w)*state.σs + state.b\n",
    "end\n",
    "\n",
    "function recalc_amp!(state::QRBMState)\n",
    "    state.amp = amplitude_rbmwf(state.σs, state.a, state.θs)\n",
    "end\n",
    "#amplitude_rmbwf(s, a, θs) = exp(dot(s, a)) * prod(cosh.(θs))\n",
    "amplitude_rbmwf(σs::Vector{Int}, a::Vector{ComplexF64}, θs::Vector{ComplexF64}) = \n",
    "    exp(dot(σs, a) + sum(log.(cosh.(θs))))\n",
    "\n",
    "### Parameters\n",
    "# `N` is the number of spins, `M` the number of hidden spins and `α=M/N` the ratio of hidden layer to visible layer\n",
    "N = 16\n",
    "α = 4\n",
    "M = floor(α * N)\n",
    "n_pars = N*M + N + M\n",
    "\n",
    "state = QRBMState(N, M)\n",
    "@show state.amp\n",
    "@show state.σs;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The optimization algorithm\n",
    "### Gradient descent\n",
    "We would like to represent the ground state of some Hamiltonian $H$ (for example the quantum Ising in transverse field) so we choose to minimize the variational energy with respect to $\\psi$ that is a function of the current weights $w$ and biases $a, b$.\n",
    "$$\n",
    "    E = \\frac{\\langle \\psi | H | \\psi \\rangle}{\\langle \\psi | \\psi \\rangle}\n",
    "$$\n",
    "The generalized force (the gradient of the variational energy) is a real-valued function of the complex weights and biases, therefore\n",
    "$$\n",
    "    F_k(w) = \\frac{\\partial}{\\partial w_k} E(w) = 2 \\frac{\\partial}{\\partial w^*_k}E(w, w^*) =\n",
    "    \\sum_{\\boldsymbol{\\sigma}} \n",
    "    \\frac{\\left[\\frac{\\partial}{\\partial w^*_k} \\psi^*(\\boldsymbol{\\sigma} )\\right]\\langle \\boldsymbol{\\sigma}  | H | \\psi \\rangle}{\\langle \\psi | \\psi \\rangle} - \n",
    "    \\sum_\\boldsymbol{\\sigma}\n",
    "    \\frac{\\psi^*(\\boldsymbol{\\sigma} )\\langle \\boldsymbol{\\sigma}  | H | \\psi \\rangle}{\\langle \\psi | \\psi \\rangle} \n",
    "    \\sum_{\\boldsymbol{\\sigma} '} \n",
    "    \\frac{\\left[\\frac{\\partial}{\\partial w^*_k} \\psi^*(\\boldsymbol{\\sigma} ')\\right]\\psi(\\boldsymbol{\\sigma} ')}{\\langle \\psi | \\psi \\rangle}\n",
    "$$\n",
    "As usual Interpreting $p(\\boldsymbol{\\sigma} ) = \\frac{|\\psi(\\boldsymbol{\\sigma} )|^2}{\\langle \\psi | \\psi \\rangle}$ as probability of configuration $\\boldsymbol{\\sigma} $ and defining the average (with some abuse of notation)\n",
    "$\\langle A\\rangle = \\sum_{\\boldsymbol{\\sigma}} p(\\boldsymbol{\\sigma}) A(\\boldsymbol{\\sigma})$ we have \n",
    "$$\n",
    "F_k(w) =  \\langle O^*_k \\mathcal{E}\\rangle - \\langle O^*_k\\rangle\\langle \\mathcal{E}\\rangle\n",
    "$$\n",
    "where the operators $O_k(\\boldsymbol{\\sigma})$ and $\\mathcal{E}(\\boldsymbol{\\sigma})$ are defined as \n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\mathcal{E}(\\boldsymbol{\\sigma} ) & = \\frac{\\langle \\boldsymbol{\\sigma}  | H | \\psi \\rangle}{\\psi(\\boldsymbol{\\sigma})}  \\\\\n",
    "        O_k(\\boldsymbol{\\sigma}) & = \\frac{\\partial_k \\psi(\\boldsymbol{\\sigma})}{\\psi(\\boldsymbol{\\sigma})} \n",
    "    \\end{aligned}\n",
    "$$\n",
    "The usual gradient descent algorithm can be implemented to update the weights and biases with learning rate $\\lambda$ according to \n",
    "$$\n",
    "    w_k^{i+1} = w_k^i - \\lambda F_k(w^i)\n",
    "$$\n",
    "If we take the \"generalized force\" name seriously, a more physical update is to use the force to change the velocity and use that to change the weights. This approach is the momentum based gradient descent where with an initial velocity of $v^0 = 0$ and the momentum memeory amplitude is typically chosen to be $\\gamma = 0.9$. Note that in physical terms $1-\\gamma$ can be interpreted as friction amplitude.\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        v^i & = \\gamma v^{i-1} + \\lambda F_k(w^i) \\\\\n",
    "        w_k^{i+1} &= w_k^i - v^i\n",
    "    \\end{aligned}\n",
    "$$\n",
    "While many different approached for the calculation of $v^{i+1}$ from $v^i$ exists, it turns out a simple calculation of the gradient in future (like in the backward Euler method) results in better convergence.\n",
    "$$\n",
    "    v^i  = \\gamma v^{i-1} + \\lambda F_k(w^i - \\gamma v^{i-1})\n",
    "$$\n",
    "\n",
    "The derivatives can be taken explicitely. Based on the definition of $\\psi(\\boldsymbol{\\sigma})$, the derivatives are given by\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\partial_{a_i} \\ln \\psi &= \\sigma_i \\\\\n",
    "        \\partial_{b_j} \\ln \\psi &= \\tanh \\theta_j \\\\\n",
    "        \\partial_{w_{ij}} \\ln \\psi &= \\sigma_i \\tanh \\theta_j\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Monte Carlo evaluation\n",
    "Since the configuration space is $2^N$ large, it is generally impossible to evaluate the above averages for problems of interest. Therefore, we should pick a sampling method to approximately evaluate the averages. At each step of the optimization, the Monte-Carlo sampling method picks a series of random configurations $\\boldsymbol{\\sigma}_1, \\dots, \\boldsymbol{\\sigma}_\\ell$ and evaluates the operators. New configurations are chosen by flipping a random spin and according to the acceptance probability \n",
    "$$\n",
    "    P_{\\text{accept}}(\\boldsymbol{\\sigma}_{\\ell+1}) = \\min \\left(1, \\left|\\frac{\\psi(\\boldsymbol{\\sigma}_{\\ell+1})}{\\psi(\\boldsymbol{\\sigma}_\\ell)}\\right|^2\\right)\n",
    "$$\n",
    "To avoid recalculations, the set of $\\theta_j$ is saved and updated only for the connection to the flipped spin.\n",
    "\n",
    "Note that this process amount to a version of *stochastic* gradient descent as the total gradient is not computed at each step. The choice of configurations, however, is chosen by a Metropolis walk rather than uniform random to respect the probability distribution induced by the wavefunction over the configurations.\n",
    "\n",
    "### Stochastic reconfiguration \n",
    "It is important to recognize that the wavefunction optimization (through its gradient) takes place in the space of weights. Therefore, instead of moving in the direction of the calculated gradient of the variational energy, one can choose to move in the direction suggested by the *power method*. In each step of the power method we apply the operator $\\Lambda I - H$ to the wavefunction. For a large enough $\\Lambda$ that insures the positivity of $\\Lambda I-H$, the power method converges closer to the ground state\n",
    "$$\n",
    "    \\psi^{t+1}_p = (\\Lambda I - H) \\psi^{t}\n",
    "$$\n",
    "In general the new wavefunction may not live entirely inside of the variational space, so we need to project it back to the variational space. Choosing a basis for the variational space\n",
    "$$\n",
    "    \\psi^{t+1}_v = \\psi^{t} + \\sum_{k=1} \\delta w_k ~\\frac{\\partial}{\\partial w_k} \\psi = \\sum_{k=0}\\delta w_k ~ O_k \\psi\n",
    "$$\n",
    "We now require the projection of the power method wavefunction and variational change wavefunction to be the same. This gives a linear equation that solves for the direction of movements.\n",
    "$$\n",
    "    \\langle \\psi\\rvert O^*_k (\\Lambda -H) \\lvert\\psi\\rangle = \\sum_l \\langle\\psi \\lvert O^*_k O_l \\rvert\\psi\\rangle ~\\delta w_l\n",
    "$$\n",
    "substituting $\\Lambda$ from the equation for $k=0$ into the equation for general $k$ we find that\n",
    "$$\n",
    "    \\langle  O^*_k \\rangle\\langle  H \\rangle-\\langle  O^*_k H \\rangle = \n",
    "    \\sum_l \\left(\\langle O^*_k O_l \\rangle - \\langle O^*_k \\rangle\\langle O_k \\rangle\\right) ~\\delta w_l\n",
    "$$\n",
    "The solution is to move in the direction \n",
    "$$\n",
    "    \\delta w_l = - s^{-1}_{lk} F_k \\qquad \\text{where} \\qquad s_{kl} = \\langle O^*_k O_l \\rangle - \\langle O^*_k \\rangle\\langle O_k \\rangle\n",
    "$$\n",
    "Note that this means we are moving in the direction of the gradient descend of the energy variation but with a new metric $s_{kl}$ !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the required Monte-Carlo functions\n",
    "function propose_update!(state::QRBMState, spin::Int)\n",
    "    σs_new = copy(state.σs)\n",
    "    σs_new[spin] *= -1\n",
    "    θs_new = state.θs + 2 * σs_new[spin] .* state.w[spin, :]\n",
    "    new_amp = amplitude_rbmwf(σs_new, state.a, θs_new)\n",
    "    if abs(new_amp/state.amp)^2 > rand()\n",
    "        state.σs = σs_new\n",
    "        state.θs = θs_new\n",
    "        state.amp = new_amp\n",
    "        return true\n",
    "    end\n",
    "    false\n",
    "end\n",
    "\n",
    "function measure(state::QRBMState, what::Symbol)\n",
    "    N = state.N\n",
    "    if what == :ZZ\n",
    "        return [state.σs[i]*state.σs[mod1(i+1, N)] for i in 1:N]\n",
    "    elseif what == :X\n",
    "        sx = zeros(ComplexF64, N)\n",
    "        for i = 1:N\n",
    "            σi = copy(state.σs)\n",
    "            σi[i] *= -1\n",
    "            sx[i] =  amplitude_rbmwf(σi, state.a, state.θs + 2 * σi[i] .* state.w[i, :])/state.amp\n",
    "        end\n",
    "        return sx\n",
    "    elseif what == :OStar\n",
    "        partial_bs = tanh.(state.θs)\n",
    "        return conj([state.σs...;partial_bs...; [state.σs[i] * partial_bs[j] for j=1:state.M for i=1:N]...])\n",
    "    end\n",
    "    error(\"unrecognized measurement!\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent RBM learning for ground state of quantum Ising transverse field\n",
    "g = 1.00\n",
    "# exact energy of the peridoic ITF per bond (N → ∞)\n",
    "itfexact(g) = -quadgk(k->sqrt(1+g^2-2*g*cos(k)), 0, pi)[1]/pi\n",
    "@show itfexact(g) * N\n",
    "\n",
    "# learning rate\n",
    "λ = 0.01\n",
    "\n",
    "n_updates = 100\n",
    "for step = 1:n_updates\n",
    "    N, M = state.N, state.M\n",
    "    n_mcsweeps = 100\n",
    "    l = 0\n",
    "    O_av    = zeros(n_pars)\n",
    "    E_av    = 0.0\n",
    "    szsz_av = zeros(N)\n",
    "    sx_av   = zeros(N)\n",
    "    EO_av   = zeros(n_pars)\n",
    "    n_samples = n_mcsweeps*N\n",
    "    counter = 0\n",
    "    while l < n_samples\n",
    "        #print(\"x\")\n",
    "        spin = rand(1:N)\n",
    "        counter += 1\n",
    "        if propose_update!(state, spin)\n",
    "            l += 1\n",
    "            szsz     = measure(state, :ZZ)\n",
    "            sx       = measure(state, :X)\n",
    "            OStar    = measure(state, :OStar)\n",
    "            E = -sum(szsz) - g*sum(sx)\n",
    "            O_av += OStar\n",
    "            E_av += E\n",
    "            EO_av += E .* OStar\n",
    "        end\n",
    "        if counter > 20*n_samples\n",
    "            error()\n",
    "        end\n",
    "    end\n",
    "\n",
    "    EO_av /= n_samples\n",
    "    O_av /= n_samples\n",
    "    E_av /= n_samples\n",
    "    F = EO_av - E_av .* O_av    \n",
    "    println(E_av)\n",
    "\n",
    "    # update weights and bias\n",
    "    state.a -= λ *F[1:N]\n",
    "    state.b -= λ *F[N+1:N+M]\n",
    "    state.w -= λ *reshape(F[N+M+1:n_pars], N, M)\n",
    "\n",
    "    recalc_θs!(state)\n",
    "    recalc_amp!(state)\n",
    "end\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
